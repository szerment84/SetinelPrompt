# SetinelPrompt
This project is a security testing tool for LLMs. It automatically performs prompt injection and jailbreak attacks, analyzes responses, and reports potential vulnerabilities. Compatible with any OpenAI/Together API model, it helps quickly assess the robustness of language models
